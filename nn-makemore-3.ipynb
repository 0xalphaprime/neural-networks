{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## makemore 3 \n",
    "\n",
    "__Batch Normalization__\n",
    "[Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)\n",
    "\n",
    "The research paper titled \"Deep Residual Learning for Image Recognition\" presents a breakthrough innovation in training deep neural networks for image recognition tasks. This innovation involves using very deep neural networks with \"shortcut connections\" (also known as \"skip connections\" or \"identity mappings\") to overcome the problem of vanishing gradients. \n",
    "\n",
    "Simply put, the problem of vanishing gradients arises when the gradients of the loss function with respect to the weights become very small as they are propagated backwards through multiple layers of a deep neural network. This problem makes it difficult to train neural networks with many layers, as the weights of the early layers do not get updated effectively. One common solution to this problem is to use a technique called \"batch normalization\" that normalizes the inputs to each layer. However, this technique has its own limitations and does not always work well in practice.\n",
    "\n",
    "The innovation presented in this paper involves using \"shortcut connections\" to enable the gradients to propagate more easily through the network. These connections allow the input of a layer to be added to the output of another layer further down the network, creating a \"residual\" of the input that can bypass one or more layers. The key insight behind this technique is that if the skipped layers can approximate an identity function, then the network can be more easily optimized to learn the desired mapping between the input and output.\n",
    "\n",
    "The authors demonstrate the effectiveness of their approach on the challenging ImageNet dataset, where they achieve state-of-the-art results using very deep neural networks with up to 152 layers. They also show that their approach is effective for a wide range of other image recognition tasks, including object detection and semantic segmentation.\n",
    "\n",
    "Overall, the paper provides a significant contribution to the field of deep learning by providing a new approach to training very deep neural networks that can optimize the learning process more effectively than previous techniques. This innovation has many potential applications in computer vision and other fields that rely on deep neural networks for learning complex mappings between inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all of the data\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocab of characters and mappings to and from integers\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # this is the length of the sequences we will use for training\n",
    "\n",
    "def build_dataset(words):\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # shift the context by one character\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random \n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(len(words) * 0.8)\n",
    "n2 = int(len(words) * 0.9)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # training set 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # dev set 10%\n",
    "Xte, Yte = build_dataset(words[n2:]) # test set 10%\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ml-050923",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
